{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KFbhhvSjOfN"
      },
      "source": [
        "# Phase 1 - Ingestion and Cleaning\n",
        "\n",
        "In the Phase 2 of the Case Study, we will carry out the following steps:\n",
        "  - Ingest raw downloaded data\n",
        "  - Output a combined dataset ready for analysis and modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W-22vI0jOfP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sys import platform\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEWV2LBEjOfQ"
      },
      "outputs": [],
      "source": [
        "# A helper function that you'll be using while reading the raw files\n",
        "def is_integer(x):\n",
        "    '''\n",
        "    This function returns True if x is an integer, and False otherwise\n",
        "    '''\n",
        "    try:\n",
        "        return (int(x) == float(x))\n",
        "    except:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQImXIIJjOfQ"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwhoxepujOfQ"
      },
      "outputs": [],
      "source": [
        "# Define the directories that contain the files downloaded\n",
        "dir_cs = \".\" # path to the directory where all the *.csv.zip files are located\n",
        "\n",
        "# Define the output path for the pickle\n",
        "pickle_file = dir_cs + \"clean_data.pickle\" # path to save cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9BDvMHmjOfQ"
      },
      "outputs": [],
      "source": [
        "# Identify the columns we'll be keeping from the dataset\n",
        "cols_to_pick = [\"id\", \"loan_amnt\",\"funded_amnt\", \"term\", \"int_rate\", \"grade\", \"emp_length\", \"home_ownership\",\"annual_inc\", \n",
        "                \"verification_status\", \"issue_d\", \"loan_status\", \"purpose\", \"dti\",\"delinq_2yrs\", \n",
        "                \"earliest_cr_line\", \"open_acc\", \"pub_rec\", \"fico_range_high\",\"fico_range_low\", \n",
        "                \"revol_bal\", \"revol_util\", \"total_pymnt\", \"recoveries\",\"last_pymnt_d\"] # list of features to use for this study as indicated in the handout\n",
        "\n",
        "# Identify the type of each of these column based on your CS-Phase 1 response\n",
        "float_cols = [\"loan_amnt\",\"funded_amnt\",\"annual_inc\", \"dti\",\"delinq_2yrs\",\"open_acc\",\"pub_rec\",\"fico_range_high\",\n",
        "              \"fico_range_low\",\"revol_bal\",\"total_pymnt\",\"recoveries\"]\n",
        "cat_cols = [\"term\",\"grade\",\"verification_status\",\"emp_length\",\"home_ownership\",\"verification_status\",\"loan_status\",\"purpose\"] # categorical features\n",
        "perc_cols = ['int_rate', 'revol_util']\n",
        "date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']\n",
        "\n",
        "# Ensure that we have types for every column\n",
        "assert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzO-eJjmjOfR"
      },
      "outputs": [],
      "source": [
        "# Some of the columns selected will not be used directly in the model,\n",
        "# but will be used to generate other features.\n",
        "#\n",
        "# Create variables specifying the features that will be used\n",
        "\n",
        "# All categorical columns other than \"loan_status\" will be used as\n",
        "# discrete features\n",
        "\n",
        "discrete_features = list(set(cat_cols) - set([\"loan_status\"]))\n",
        "\n",
        "# All numeric columns will be used as continuous features\n",
        "continuous_features = list(float_cols + perc_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPyJh8WmjOfR"
      },
      "source": [
        "## Ingestion\n",
        "Ingest the data files from both sets, perform consistency checks, and prepare one single file for each set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrFfJ5UnjOfR",
        "outputId": "876b2c4d-edd4-4d10-f50f-433f2f0542de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(dir_cs) if isfile(join(dir_cs, f))]\n",
        "onlyfiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00mqRc9sjOfS"
      },
      "outputs": [],
      "source": [
        "def ingest_files(directory):\n",
        "    '''\n",
        "    This function will ingest every file in the specified directory\n",
        "    into a pandas dataframe. It will return a dictionary containing\n",
        "    these dataframes, keyed by the file name.\n",
        "    \n",
        "    We assume the directory contains files directly downloaded from\n",
        "    the link given in the handout, and *only* those files. Thus, we \n",
        "    assume the files are zipped (pd.read_csv can read zipped files) \n",
        "    and we assume the first line in each file needs to be skipped.\n",
        "    \n",
        "    Note that each file will be read *without* formatting\n",
        "    '''\n",
        "    \n",
        "    # If the directory has no trailing slash, add one\n",
        "    if directory[-1] != \"/\":\n",
        "        directory+\"/\"\n",
        "    \n",
        "    all_files = [f for f in listdir(directory) if isfile(join(directory, f))] # get list of all files from the directory\n",
        "    output = {}\n",
        "    \n",
        "    print(\"Directory \" + directory + \" has \" + str(len(all_files)) + \" files:\")\n",
        "    for i in all_files:\n",
        "        print(\"    Reading file \" + i)\n",
        "        output[i] = pd.read_csv(directory+i,skiprows=1,dtype='str') # read each with dtype='str' and skip_rows =1\n",
        "        df=output[i]\n",
        "        \n",
        "        # Some of the files have \"summary\" lines that, for example\n",
        "        # read \"Total number of loans number in Policy 1: .....\"\n",
        "        # To remove those lines, find any lines with non-integer IDs\n",
        "        # and remove them\n",
        "        invalid_rows = np.array([0 if is_integer(id) else 1 for id in df[\"id\"]]) # mask rows that have non-integer IDs. Use is_integer method\n",
        "        \n",
        "        if invalid_rows.sum() > 0:\n",
        "            print(\"Found \" + str(invalid_rows.sum()) + \" invalid rows which were removed\")\n",
        "            output[i] = df[~invalid_rows.astype(bool)] # remove invalid rows\n",
        "    \n",
        "    return output # return dictionary of dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je4rwaAijOfS",
        "outputId": "544d4fdb-3080-4f22-d3a7-e82b7070d451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory . has 0 files:\n"
          ]
        }
      ],
      "source": [
        "# Ingest the set of files we downloaded using the defined method \"ingest_files\"\n",
        "files_cs = ingest_files(dir_cs) # dictioary of (filename, dataframe) as (key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE0mloyUjOfS"
      },
      "source": [
        "### Combine the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqDIJL3bjOfS",
        "outputId": "711e7600-74a2-464f-ddbe-a91d4079d698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4a9fc35e8d94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_cs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_cs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# combine \"files_cs\" into a pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_cs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_cs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# resent index with drop = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "\n",
        "data_cs = pd.concat([value for value in files_cs.values()]) # combine \"files_cs\" into a pandas dataframe\n",
        "data_cs.head()\n",
        "data_cs.reset_index(drop=True)             # resent index with drop = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tyhG6qrjOfS"
      },
      "source": [
        "## Prepare Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOo7RNl4jOfT"
      },
      "outputs": [],
      "source": [
        "# Keep only the columns of interest from 'data_cs'\n",
        "final_data = data_cs[cols_to_pick]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGvJvwcBjOfT"
      },
      "outputs": [],
      "source": [
        "print(\"Starting with \" + str(len(final_data)) + \" rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isldwn8mjOfT"
      },
      "source": [
        "### Typecast the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk6VK-jWjOfT"
      },
      "outputs": [],
      "source": [
        "# Remember that we read the data as string (without any formatting). \n",
        "# Now we would typecast the columns based on feature types which you found out in CS Phase 1\n",
        "\n",
        "for i in float_cols:\n",
        "    final_data[i] = final_data[i].astype(float) # typecast float columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmBV-bQdjOfT"
      },
      "outputs": [],
      "source": [
        "def clean_perc(x):\n",
        "    if pd.isnull(x):\n",
        "        return np.nan\n",
        "    else:\n",
        "        return float(x.strip()[:-1])\n",
        "\n",
        "for i in perc_cols:\n",
        "    final_data[i] = [clean_perc(x) for x in final_data[i]] # apply clean_perc to percentage columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcJ_3XMPjOfT"
      },
      "outputs": [],
      "source": [
        "def clean_date(x):\n",
        "    if pd.isnull(x):\n",
        "        return None\n",
        "    else:\n",
        "        return datetime.datetime.strptime( x, \"%b-%Y\").date()\n",
        "\n",
        "for i in date_cols:\n",
        "    final_data[i] = [clean_date(d) for d in final_data[i]] # typecast date cloumns to datatime using clean_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ppcJclcjOfT"
      },
      "outputs": [],
      "source": [
        "for i in cat_cols:\n",
        "    final_data[i]=final_data[i].where(pd.notnull(final_data[i]),None) # for categorical features if the value is null/empty set it to None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4SOfKHujOfU"
      },
      "source": [
        "## Calculate returns for each loan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csbQcHXMjOfU"
      },
      "outputs": [],
      "source": [
        "# Define the names of the four returns we'll be calculating as described in Q.6\n",
        "# ret_PESS: Pessimistic return\n",
        "# ret_OPT: Optimistic return\n",
        "# ret_INTa, ret_INTb: Method3 at two differnt values of \"i\"\n",
        "ret_cols = [\"ret_PESS\", \"ret_OPT\", \"ret_INTa\", \"ret_INTb\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CplqHc3jOfU"
      },
      "outputs": [],
      "source": [
        "# Remove all rows for loans that were paid back on the days they were issued\n",
        "final_data['loan_length'] = (final_data.last_pymnt_d - final_data.issue_d) / np.timedelta64(1, 'M')\n",
        "n_rows = len(final_data)\n",
        "\n",
        "final_data = final_data[final_data[\"loan_length\"]!=0] # select rows where loan_length is not 0. \n",
        "\n",
        "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkQgx82YjOfU"
      },
      "source": [
        "### M1-Pessimistic Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hely3u3MjOfU"
      },
      "outputs": [],
      "source": [
        "# Calculate the return using a simple annualized profit margin\n",
        "# Pessimistic definition (Handout 6a.) (M1)\n",
        "\n",
        "final_data['term_num'] = final_data.term.str.extract('(\\d+)',expand=False).astype(int) # length of loan in months\n",
        "\n",
        "final_data['ret_PESS'] = ((final_data[\"total_pymnt\"]-final_data[\"loan_amnt\"])/final_data[\"loan_amnt\"])*(12/final_data[\"term_num\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujrtDqCqjOfU"
      },
      "source": [
        "### M2-Optimistic Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY-ViQZ4jOfU"
      },
      "outputs": [],
      "source": [
        "# Assuming that if a loan gives a positive return, we can\n",
        "# immediately find a similar loan to invest in; if the loan\n",
        "# takes a loss, we use M1-pessimistic to compute the return\n",
        "final_data['m2_months'] = np.round(((final_data[\"last_pymnt_d\"]-final_data[\"issue_d\"])/np.timedelta64(1, 'M')),0)\n",
        "\n",
        "final_data['ret_OPT'] = ((final_data[\"total_pymnt\"]-final_data[\"loan_amnt\"])/final_data[\"loan_amnt\"])*(12/final_data[\"m2_months\"])\n",
        "\n",
        "final_data.loc[final_data.ret_OPT < 0,'ret_OPT'] = ((final_data.loc[final_data.ret_OPT < 0,\"total_pymnt\"]-final_data.loc[final_data.ret_OPT < 0,\"loan_amnt\"])/final_data.loc[final_data.ret_OPT < 0,\"loan_amnt\"])*(12/final_data.loc[final_data.ret_OPT < 0,\"term_num\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lx2WL8SjOfV"
      },
      "source": [
        "### Method 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7f4BNJPjOfV"
      },
      "outputs": [],
      "source": [
        "def ret_method_3(T, i):\n",
        "    '''\n",
        "    Given an investment time horizon (in months) and re-investment\n",
        "    interest rate, calculate the return of each loan\n",
        "    '''\n",
        "    \n",
        "    # Assuming that the total amount paid back was paid at equal\n",
        "    # intervals during the duration of the loan, calculate the\n",
        "    # size of each of these installment\n",
        "    actual_installment = (final_data.total_pymnt - final_data.recoveries) / final_data[\"loan_length\"]\n",
        "    # Assuming the amount is immediately re-invested at the prime\n",
        "    # rate, find the total amount of money we'll have by the end\n",
        "    # of the loan\n",
        "    cash_by_end_of_loan = actual_installment * (1-np.power(1+i,final_data[\"loan_length\"])/1-(1+i)) #Compute the quantity given in [] in eq.2.3 of handout\n",
        "    \n",
        "    cash_by_end_of_loan = cash_by_end_of_loan + final_data.recoveries\n",
        "    \n",
        "    # Assuming that cash is then re-invested at the prime rate,\n",
        "    # with monthly re-investment, until T months from the start\n",
        "    # of the loan\n",
        "    remaining_months = T - final_data['loan_length']\n",
        "    final_return = cash_by_end_of_loan * np.power(1+i,remaining_months) - final_data[\"loan_amnt\"]\n",
        "\n",
        "    # Find the percentage return\n",
        "    ret_val = (12/T) * final_return*(1/final_data[\"loan_amnt\"])\n",
        "    return ret_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61mAW7W_jOfV"
      },
      "outputs": [],
      "source": [
        "final_data['ret_INTa'] = ret_method_3(60,0.023) # call ret_method_3 with T=60, i=0.023\n",
        "final_data['ret_INTb'] = ret_method_3(60,0.04) # call ret_method_3 with T=60, i=0.04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlrvztdCjOfV"
      },
      "outputs": [],
      "source": [
        "final_data[[\"ret_INTa\",\"ret_INTb\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4rbFBpAjOfV"
      },
      "source": [
        "### Visualize the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zigp_M27jOfV"
      },
      "outputs": [],
      "source": [
        "def visualize_float_columns():\n",
        "    '''\n",
        "    This function visualizes Box-and-whisker plots for continuous variables\n",
        "    '''\n",
        "    \n",
        "    # FLoat columns\n",
        "    for i in float_cols + perc_cols + ret_cols:\n",
        "        seaborn.boxplot(final_data[i])\n",
        "\n",
        "        # Print the three highest values\n",
        "        #highest_vals = final_data[i].sort_values()[:3] # get 3 highest values\n",
        "        highest_vals = final_data[i].sort_values(ascending=False)[:3].values # get 3 highest values\n",
        "        print(highest_vals)\n",
        "        \n",
        "        smallest_val = min(final_data[i])\n",
        "        \n",
        "        plt.text(smallest_val, -0.3, highest_vals[0])\n",
        "        plt.text(smallest_val, -0.2, highest_vals[1])\n",
        "        plt.text(smallest_val, -0.1, highest_vals[2])\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-RsBS9QjOfV"
      },
      "outputs": [],
      "source": [
        "def visualize_cat_columns():\n",
        "    '''\n",
        "    Lists the distinct values for categorical columns\n",
        "    '''\n",
        "    # Categorical columns \n",
        "    for i in cat_cols:\n",
        "        print(i) # print field name\n",
        "        print(final_data[i].unique()) # print number of distinct values\n",
        "        print(final_data[i].value_counts()) # for each distinct value print the number of occurances\n",
        "        print(\"\")\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6j8LI-6jOfV"
      },
      "outputs": [],
      "source": [
        "def visualize_date_columns():\n",
        "    '''\n",
        "    This function visualizes a timeline density for dates\n",
        "    '''\n",
        "    \n",
        "    # Date columns\n",
        "    for i in date_cols:\n",
        "        final_data[final_data[i].isnull() == False][i].apply(lambda x : str(x.year) +\n",
        "                                                \"-\" + str(x.month)).value_counts(ascending = True).plot()\n",
        "        plt.title(i + \" (\" + str(final_data[i].isnull().sum()) + \" null values)\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "hsg0op04jOfW"
      },
      "outputs": [],
      "source": [
        "# visualize continuous features\n",
        "visualize_float_columns()\n",
        "\n",
        "# visulaize categorical features\n",
        "visualize_cat_columns()\n",
        "\n",
        "# visualize date columns\n",
        "visualize_date_columns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoevzQtdjOfW"
      },
      "source": [
        "### Handle outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoEU51NXjOfW"
      },
      "outputs": [],
      "source": [
        "# There are quite a few outliers. \n",
        "# Please identify top-k (decide this based on the visualization) features where outliers are most obvious\n",
        "n_rows = len(final_data)\n",
        "#df[np.abs(df.Data-df.Data.mean()) <= (3*df.Data.std())]\n",
        "\n",
        "final_data = final_data[np.abs(final_data[\"annual_inc\"]-final_data[\"annual_inc\"].mean())<=(3*final_data[\"annual_inc\"].std())] # remove outliers based 1st obvious feature\n",
        "final_data = final_data[np.abs(final_data[\"dti\"]-final_data[\"dti\"].mean())<=(3*final_data[\"dti\"].std())] # remove outliers based 2nd obvious feature\n",
        "final_data = final_data[np.abs(final_data[\"delinq_2yrs\"]-final_data[\"delinq_2yrs\"].mean())<=(3*final_data[\"delinq_2yrs\"].std())]\n",
        "final_data = final_data[np.abs(final_data[\"pub_rec\"]-final_data[\"pub_rec\"].mean())<=(3*final_data[\"pub_rec\"].std())]\n",
        "final_data = final_data[np.abs(final_data[\"revol_bal\"]-final_data[\"revol_bal\"].mean())<=(3*final_data[\"revol_bal\"].std())]\n",
        "final_data = final_data[np.abs(final_data[\"recoveries\"]-final_data[\"recoveries\"].mean())<=(3*final_data[\"recoveries\"].std())]\n",
        "final_data = final_data[np.abs(final_data[\"open_acc\"]-final_data[\"open_acc\"].mean())<=(3*final_data[\"open_acc\"].std())] # remove outliers based kth obvious feature\n",
        "\n",
        "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWJLQTq8jOfW"
      },
      "outputs": [],
      "source": [
        "# Remove all loans that are still current\n",
        "n_rows = len(final_data)\n",
        "\n",
        "#final_data = final_data[[final_data[\"loan_status\"]!=\"Fully Paid\" and final_data[\"loan_status\"]!=\"Charged-Off\" and final_data[\"loan_status\"]!=\"Default\"]]\n",
        "final_data=final_data[~final_data[\"loan_status\"].isin([\"Fully Paid\",\"Charged-Off\",\"Default\"])]\n",
        "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXBygAFPjOfW"
      },
      "outputs": [],
      "source": [
        "# Only include loans isssued since 2010\n",
        "n_rows = len(final_data)\n",
        "final_data = final_data[final_data[\"issue_d\"].astype(str)>=\"2010-01-01\"]\n",
        "\n",
        "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BsorK78jOfW"
      },
      "source": [
        "### Drop null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "afIZTW6rjOfW"
      },
      "outputs": [],
      "source": [
        "# Deal with null values. We allow cateogrical variables to be null\n",
        "# OTHER than grade, which is a particularly important categorical.\n",
        "# All non-categorical variables must be non-null, and we drop\n",
        "# rows that do not meet this requirement\n",
        "\n",
        "required_cols = set(cols_to_pick) - set(cat_cols) - set([\"id\"])\n",
        "required_cols.add(\"grade\")\n",
        "\n",
        "n_rows = len(final_data)\n",
        "\n",
        "... # drop rows that contain null based only on \"required_cols\"\n",
        "\n",
        "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfmPE2d4jOfW"
      },
      "source": [
        "### Visualize clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JYIp2V1ojOfW"
      },
      "outputs": [],
      "source": [
        "# Visualize the data again after cleaning\n",
        "...\n",
        "...\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "teVCZxMLjOfX"
      },
      "outputs": [],
      "source": [
        "# Visualize the feature correlations\n",
        "    # You can compute the correlation among features and display a heat-map of the matrix \n",
        "    # OR use sns scatter or pairplot\n",
        "... \n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Yj5NiFDJjOfX"
      },
      "outputs": [],
      "source": [
        "# Visualize relation between loan status and features\n",
        "... # sns pairplot or scatter plot. Refer to recitations\n",
        "...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nvE3FmrjOfX"
      },
      "source": [
        "What do you observe after removing the outliers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5YH_M_kjOfX"
      },
      "source": [
        "### Data Exploration\n",
        "Solution to Q.7 from the handout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zs4RmIu-jOfX"
      },
      "outputs": [],
      "source": [
        "# Find the percentage of loans by grade, the default by grade,\n",
        "# and the return of each grade\n",
        "perc_by_grade = (final_data.grade.value_counts()*100/len(final_data)).sort_index()\n",
        "\n",
        "default_by_grade = final_data.groupby(\"grade\").apply(lambda x : (x.loan_status != \"Fully Paid\").sum()*100/len(x) )\n",
        "ret_by_grade_OPT = ... # average return for M2-Optimistic for each loan grade\n",
        "ret_by_grade_PESS = ... # average return for M1-Pessimistic for each loan grade\n",
        "ret_by_grade_INTa = ... # average return for M3\n",
        "ret_by_grade_INTb = ... # average return for M3\n",
        "int_rate_by_grade = ... # average interest rate for each grade\n",
        "\n",
        "combined = pd.DataFrame(perc_by_grade)\n",
        "combined.columns = ['perc_of_loans']\n",
        "combined['perc_default'] = default_by_grade\n",
        "combined['avg_int_rate'] = int_rate_by_grade\n",
        "combined['return_OPT'] = ret_by_grade_OPT\n",
        "combined['return_PESS'] = ret_by_grade_PESS\n",
        "combined['return_INTa'] = ret_by_grade_INTa\n",
        "combined['return_INTb'] = ret_by_grade_INTb\n",
        "\n",
        "combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWDG3aZUjOfX"
      },
      "source": [
        "Based on the output of previous cell, write down your answers to Q.7 from the handout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK5Jg_2GjOfX"
      },
      "source": [
        "### Save a Pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aS2uGugzjOfX"
      },
      "outputs": [],
      "source": [
        "# Remove the \"total_pymnt\" and \"recoveries\" from the list of continuous features\n",
        "continuous_features = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQXjBqsEjOfX"
      },
      "source": [
        "Why did we remove `total_pymt` and `recoveries` from the data for the task of predicting whether to give loan or not, although these are highly predictive features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6lc79ES3jOfX"
      },
      "outputs": [],
      "source": [
        "# save the prepared data for modeling in next Phase.\n",
        "pickle.dump( [final_data, discrete_features, continuous_features, ret_cols], open(pickle_file, \"wb\") )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Isldwn8mjOfT",
        "MkQgx82YjOfU",
        "ujrtDqCqjOfU",
        "4lx2WL8SjOfV",
        "i4rbFBpAjOfV",
        "yoevzQtdjOfW",
        "9BsorK78jOfW",
        "UfmPE2d4jOfW",
        "P5YH_M_kjOfX",
        "CK5Jg_2GjOfX"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}